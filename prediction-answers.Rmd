---
title: "Prediction: Comparing Tress"
author: "Zhongyuan Zhang"
date: "3/9/2020"
output: html_document
---

In this project you will model student data using three flavors of tree algorithm: CART, C4.5 and C5.0. We will be using these algorithms to attempt to predict which students drop out of courses. Many universities have a problem with students **over-enrolling** in courses at the beginning of semester and then dropping most of them as the make decisions about which classes to attend. This makes it difficult to plan for the semester and allocate resources. However, schools don't want to restrict the choice of their students. One solution is to create predictions of which students are likely to drop out of which courses and use these predictions to inform semester planning. 

We will be using the tree algorithms to build models of which students are likely to drop out of which classes. 

## Software

In order to generate our models we will need several packages. The first package you should install is [caret](https://cran.r-project.org/web/packages/caret/index.html). (https://topepo.github.io/caret/train-models-by-tag.html)(https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)

There are many prediction packages available and they all have slightly different syntax. caret is a package that brings all the different algorithms under one hood using the same syntax. 

We will also be accessing an algorithm from the [Weka suite](https://www.cs.waikato.ac.nz/~ml/weka/). Weka is a collection of machine learning algorithms that have been implemented in Java and made freely available by the University of Waikato in New Zealand. To access these algorithms you will need to first install both the [Java Runtime Environment (JRE) and Java Development Kit](http://www.oracle.com/technetwork/java/javase/downloads/jre9-downloads-3848532.html) on your machine. You can then then install the [RWeka](https://cran.r-project.org/web/packages/RWeka/index.html) package within R.


The last package you will need is [C50](https://cran.r-project.org/web/packages/C50/index.html)

## Data

The data comes from a university registrar's office. The code book for the variables are available in the file code-book.txt. Examine the variables and their definitions.

### Drop Out Code Book

student_id = Student ID
years = Number of years the student has been enrolled in their program of study
entrance_test_score = Entrance exam test score
courses_taken = Number of courses a student has taken during their program
complete = Whether or not a student completed a course or dropped out (yes = completed)
enroll_data_time = Date and time student enrolled in POSIXct format
course_id = Course ID
international = Is the student from overseas
online = Is the student only taking online courses
gender = One of five possible gender identities


Upload the drop-out.csv data into R as a data frame. 


```{r}
library(tidyverse)
D1 <- read.csv("drop-out.csv", header = TRUE)
D1$complete <- as.factor(D1$complete)
str(D1)

#D1$enroll_date_time<-as.POSIXct(D1$enroll_date_time,origin=Sys.Date())
```

The next step is to separate your data set into a training set and a test set. Randomly select 25% of the students to be the test data set and leave the remaining 75% for your training data set. (Hint: each row represents an answer, not a single student.)

```{r}
set.seed(666)

#calculating number of 75% of the students
round(length(unique(D1$student_id))*0.75)

SAMPLE <- data.frame(sample(unique(D1$student_id), 512))
names(SAMPLE) <- c("student_id")

#adding a dataframe with only the id with inner join to get the rows with common id; anti_inner for the rest
TRAIN1 <- inner_join(D1, SAMPLE, by = "student_id")  # by using join, we can easily match those duplicated id
TEST1 <- anti_join(D1, SAMPLE, by = "student_id")


```

For this project we will be predicting the student level variable "complete". 
(Hint: make sure you understand the increments of each of your chosen variables, this will impact your tree construction)

```{r}
# Recoding variables:
## DV complete: numeric 1, 0
## IV international: numeric 1, 0
## IV online: numeric 1, 0
D2 <- D1 %>% 
  mutate(complete=as.factor(ifelse(complete=="yes", 1,0))) %>% 
  mutate(international=as.factor(ifelse(international=="yes", 1,0))) %>% 
  mutate(online=as.factor(ifelse(online=="yes", 1,0)))

```

Visualize the relationships between your chosen variables as a scatterplot matrix.  Save your image as a .pdf named scatterplot_matrix.pdf. Based on this visualization do you see any patterns of interest? Why or why not?

```{r}
png("scatterplot_matrix.png",width = 1000, height = 950)
plot(D1[,c(2:5,8:10)])
dev.off()

plot(D1[,c(2:5,8:10)])
# 1. "years" and "complete": As observed, the fewer years student spent in the program, the more likely he or she would complete. Thus, there exists a possibile negative relationship between these two variables.

# 2. "entrance_test_score" and "complete":  As observed, the higher entrance test score student has, the more likely he or she would complete.

# 3. "course_taken" and "complete": As observed, the fewer courses student took, there is higher chance he or she would complete.

# 4. "entrance_test_score" and "courses_taken": there is a seemingly negative relationship, that students who have taken more courses are more likely to have a lower entrance exam test score.

# 5. "years" and "entrance_test_score": As observed, students with with higher entrance exam test score spent fewer years in the program.

# 6. "years" and "online" : As observed, students who have taken online courses spent fewer years in the program.

# 7. "years" and "international" : As observed, international students have taken fewer years than non-international students.
```

## CART Trees

In HUDK4050 we used the [rpart package](https://cran.r-project.org/web/packages/rpart/rpart.pdf) to generate CART tree models. But here we use caret.
```{r}
library(rpart)
library(rpart.plot)
TRAIN2 <- TRAIN1[,c(2:10)]#Remove the student_id variable that we do not want to use in the model


rpart_model <- rpart(complete ~ ., data = TRAIN2, method = "class", control = rpart.control(cp = 0))
rpart.plot(rpart_model)
plotcp(rpart_model)
printcp(rpart_model)

# Make predictions on the test dataset
TEST2 <- TEST1[,c(2:10)]
TEST2$pred <- predict(rpart_model, TEST2, type = "class")


# Examine the confusion matrix
table(TEST2$pred, TEST2$complete)


# Compute the accuracy on the test dataset
mean(TEST2$pred==TEST2$complete) #87.4%

mean(predict(rpart_model, TRAIN2, type = "class")==TRAIN2$complete) #91.7% #

#---------------------------------------------------------------------------------------------------------
#Based on the plotcp, we are considering pruning the tree at a cp of 0.0053 to somehow increase the predictability of the model
rpart_model_pruned<-prune(rpart_model,cp=0.0053)

TEST2$pred_pru <- predict(rpart_model_pruned, TEST2, type = "class")

# Compute the accuracy on the test dataset
mean(TEST2$pred_pru==TEST2$complete) #89.9%

mean(predict(rpart_model_pruned, TRAIN2, type = "class")==TRAIN2$complete) #89.79 the tree is less fitted to the training data, but more generalizable after pruning.
```


Construct a classification tree that predicts complete using the caret package.



```{r}
library(caret)

TRAIN2 <- TRAIN1[,c(2:10)] 
#Define the control elements we would like to use
ctrl <- trainControl(method = "repeatedcv", #Tell caret to perform 10-fold cross validation
                repeats = 3, #Tell caret to repeat each fold three times
                classProbs = TRUE, #Calculate class probabilities for ROC calculation
                summaryFunction = twoClassSummary)

#Define the model
cartFit <- train(complete ~ ., #Define which variable to predict 
                data = TRAIN2, #Define the data set to train the model on
                trControl = ctrl, #Tell caret the control elements (EG - the cross validation)
                method = "rpart", #Define the model type; no need to manually set up the pruning paramenter
                metric = "ROC", #Tell caret to calculate the ROC curve
                preProc = c("center", "scale")) #Prepocessing the data: Centering and scaling

#Check the results
cartFit
                
#Plot ROC against complexity 
plot(cartFit)

```

Describe important model attribues of your tree. Do you believe it is a successful model of student performance, why/why not?

First, The final value used for the model was cp = 0.01026046. The corresponding value of ROC to this cp is 0.8894181. This means that there is a 88.94% probability of a randomly selected student from a "completed" group being positioned as "completed"(TP) to a randomly selected student from a "not completed" group being positioned as "completed"(FP). This ROC value is fairly high. 

Second, the specificity, which is  true negative  / true positive + true negative, of the model is 0.9953719. This means that the model captures the dropout students pretty well since 99.54% of the non-complete cases were correctly predicted.

However, the sensitivity, which is  true positive  / true positive + true negative, of the model is 0.6569512. This means that the model was moderately good to capture the complete cases. Only 65.69% of the complete cases were accurately predicted.This implies that some students ~34%, who have completed the course, where incorrectly classified as "not completed". In this case, it can be said that this model of student performance is not so successful. 


What does the plot represent? What information does this plot tell us?

The plot represents the relationship between complexity parameter (e.g.:used to control the size of the decision tree and to select the optimal tree size) and ROC (e.g.: cross-validated accuracy). The more complex the decision tree is, the more accurate the prediction is. 

This plot shows the value of ROC for each complexity parameter (cp) value or threshold. The complexity parameter (cp) is the minimum improvement in the model needed at each node. As the cp value increases, the ROC decreases. That is, the fewer nodes in a tree, the poorer the prediction. The cost of adding another variable to the decision tree from the current node is above the value of cp, thus tree building does not continue.



Now predict results from the test data and describe import attributes of this test. Do you believe it is a successful model of student performance, why/why not?


```{r}
TEST3 <- TEST1[,c(2:10)] #Remove the student_id variable that we do not want to use in the model

#Generate prediction using previously trained model
cartClasses <- predict(cartFit, newdata = TEST3)

#Generate model statistics
confusionMatrix(data = cartClasses, TEST3$complete)

# Compute the accuracy on the test dataset
mean(cartClasses==TEST3$complete) #89.5%
```

The accuracy is 0.895, which means that the decision tree can predict 89.5% of the test data correctly. The p-value is small, which indicates the result rejects the null hypothesis and is statistically significant.

 
## C4.5-Type Trees

In this part we will repeat the same prediction but using a different tree-based algorithm called [J48](). J48 is a Java implementation of the C4.5 decision tree algorithm of [Quinlan (1993)](https://link.springer.com/article/10.1007%2FBF00993309). 

How does the C4.5 algorithm differ from the CART algorithm?

Train the J48 model on the same training data and examine your results.
```{r}
ctrl <- trainControl(method = "repeatedcv",
                repeats = 3,
                classProbs = TRUE,
                summaryFunction = twoClassSummary)
library(RWeka)

j48Fit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "J48",
                metric = "ROC",
                preProc = c("center", "scale"))

#check the results 
j48Fit

#Plot ROC against confidence threshold and plot it out
pdf("J48fit.pdf")
plot(j48Fit)
dev.off()
```
Describe important model attributes of your tree. Do you believe it is a successful model of student performance, why/why not?

Data reprocessing manipulates data before training them by centering and scaling. Trcontrol defines how this funciton acts. Metric which is a measurement of the model performance is ROC. The final values used for the model were C = 0.5 and M = 3, which has the highest validity among all of the alternatives. It performs well on the training set.

What does the plot represent? What information does this plot tell us?
It represents the relationship between confidence threshold and ROC validity. When condidence threshold reach the turning point, the validity grows rapidly and reach the peak at .5


Now test your new J48 model by predicting the test data and generating model fit statistics.

```{r}

j48Classes <- predict(j48Fit, newdata = TEST2)

confusionMatrix(data = j48Classes, TEST2$complete)

#accuracy
mean(j48Classes==TEST2$complete) # 88.9%
```
The accuracy is 0.889, which is high. The p-value is low, which means that the prediction model is statistically significant.

## Alternative to Weka
Train a Conditional Inference Tree using the `party` package on the same training data and examine your results.
```{r}
ctrl <- trainControl(method = "repeatedcv",
                repeats = 3,
                classProbs = TRUE,
                summaryFunction = twoClassSummary)

ctreeFit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "ctree",
                metric = "ROC",
                preProc = c("center", "scale"))

ctreeFit

plot(ctreeFit)
```
Describe important model attribues of your tree. Do you believe it is a successful model of student performance, why/why not?

First, the final mincriterion value that was used for this model was 0.5 . The ROC value of this model is 0.9124419, which means that there is a 91.24% probability of a randomly selected student from a "completed" group being positioned as "completed" to a randomly selected student from a "not completed" group being positioned as "completed". This ROC value is fairly high, and higher than that of the CART model.

Second, the specificity, which is  true negative  / true positive + true negative, of the model is 0.9943796 This means that the proportion of negative identifications of students who have not completed was actually correct is 99.43%. 

However, the sensitivity, which is  true positive  / true positive + true negative, of the model is 0.6729930 This means that the proportion of positive identifications of students who have completed was actually correct is only 67.29%.


What does the plot represent? What information does this plot tell us?

The plot shows a peak at the middle of x-axis, indicating that when mincriterion equals to .5, the ROC can reach its highest value and making it the optimal cutoff for this model in predicting the "complete" outcome.


Now test your new Conditional Inference model by predicting the test data and generating model fit statistics.
```{r}
#Generate prediction using previously trained model
ctreeClasses <- predict(ctreeFit, newdata = TEST2)

#Generate model statistics
confusionMatrix(data =ctreeClasses, TEST2$complete)

mean(ctreeClasses==TEST2$complete) # 89.5%
```
The accuracy is 0.895, which is high. The p-value is low, which means that the prediction model is statistically significant. Almost the same as the result from CARET

##The C50 Tree

Accuracy: The C5.0 rulesets have noticeably lower error rates on unseen cases for the sleep and forest datasets. The C4.5 and C5.0 rulesets have the same predictive accuracy for the income dataset, but the C5.0 ruleset is smaller.

Speed: C5.0 is much faster; it uses different algorithms and is highly optimized. For instance, C4.5 required more than eight hours to find the ruleset for forest, but C5.0 completed the task in under three minutes

Memory: C5.0 commonly uses an order of magnitude less memory than C4.5 during ruleset construction. For the forest dataset, C4.5 needs more than 3GB but C5.0 requires less than 200MB.

C5.0's trees are noticeably smaller on the tree sizes and computation times.
C5.0 supports boosting with any number of trials, with more trials generally yielding further improvements.Boosting is a technique for generating and combining multiple classifiers to improve predictive accuracy.

```{r}
library(C50)
c50Fit <- train(complete ~ .,
                data = TRAIN2,
                trControl = ctrl,
                method = "C5.0",
                metric = "ROC",
                preProc = c("center", "scale"))


#Check the result
c50Fit

#Plot the result
pdf('c50Fit.pdf')
plot(c50Fit)
dev.off()

#Prediction
c50Classes <- predict(c50Fit, newdata = TEST2)

confusionMatrix(data = c50Classes, TEST2$complete)

mean(c50Classes==TEST2$complete) #0.893
```
First, the final alues used for the model were trials = 20, model = tree and winnow = FALSE.The corresponding ROC value of this model is 0.9240750, which means that there is a 92.41% probability of a randomly selected student from a "completed" group being positioned as "completed" to a randomly selected student from a "not completed" group being positioned as "completed". This ROC value is fairly high, and higher than those of previous models.

Second, the specificity, which is  true negative  / true positive + true negative, of the model is 0.9899719 This means that the proportion of negative identifications of students who have not completed was actually correct is 99.00%. There is a slight decrease frm those of the previous models.

However, the sensitivity, which is  true positive  / true positive + true negative, of the model is 0.6722014 This means that the proportion of positive identifications of students who have completed was actually correct is only 67.22%. There is barely any difference from those of previous models.



## Compare the models

caret allows us to compare all four models at once, excluding the pruning one constructed in rpart

```{r}
resamps <- resamples(list(cart = cartFit,c45=j48Fit, ctree = ctreeFit, c50 = c50Fit))
summary(resamps)
```

What does the model summary tell us? Which model do you believe is the best?

The summary gives us the 5 number summary of each model regarding to ROC, Sensitivity and Specificity.
As for the best model, it should be performing most stably with less variance in the parameters. Here I use eyeballing and mean as equipment to determine which model is stabler. 

In Specificity, three models differ no significantly.
In Sensitivity and ROC, ctree and c50 are slightly better than cart.

Horizontally inspected, c50 model is generally more stable than other models in ROC, Sensitivity and Specificity.
To sum up, I think the c50 model is the best.

There are mainly three parts in the summary: the metrics ROC, sensitivity, and specificity for each of the models. C5.0 model has the highest average ROC, Ctree model has the highest average sensitivity, and the CART model has the highest average specificity. The CART model has the lowest average ROC and sensitivity. 


Which variables (features) within your chosen model are important, do these features provide insights that may be useful in solving the problem of students dropping out of courses?

```{r}
#looking at variables that are important in the model.
variables <- varImp(c50Fit)
plot(variables) 

png('Important variables.png')
plot(variables) 
dev.off()

```
It shows in the plot that "years" and "course_id", which all make sense since the longer time a student have spent in a program, a more possible learning difficulty is indicated; some courses are not so "student friendly" that fail student easily; 
